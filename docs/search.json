[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a test of using Quarto to host tutorials/walkthroughs in Python and Julia"
  },
  {
    "objectID": "python_tutorials/python_example.html",
    "href": "python_tutorials/python_example.html",
    "title": "Python Notebook demo",
    "section": "",
    "text": "This notebook uses a Jupyter notebook to produce this page. This was written/edited using vscode\n\n\n(Most of this is vscode specific)\nFrom the terminal create a virtual environment in the python_tutorials directory:\npython -m venv venv\nsource venv/bin/activate\nInstall required modules:\npython -m pip install jupyter ipykernel jupyterlab\n(restart vscode if you did the above in a vscode terminal)\nLoad/create the notebook and set the notebook kernel to the vnenv you’ve created (button at the top right of the notebook)\n\n\n\nThis is just cut and pasted from https://jgori-ouistiti.github.io/CoopIHC/guide/learning.html\nI couldn’t get most of the examples to work in the documentation\n\n\nCode\nfrom coopihc.examples.simplepointing.envs import SimplePointingTask\nfrom coopihc.examples.simplepointing.users import CarefulPointer\nfrom coopihc.examples.simplepointing.assistants import ConstantCDGain\nfrom coopihc import State, Bundle\n\n\ntask = SimplePointingTask(gridsize=31, number_of_targets=8)\n\nunitcdgain = ConstantCDGain(1)\n\n\n# The policy to be trained has the simple action set [-5,-4,-3,-2,-1,0,1,2,3,,4,5]\n\naction_state = State()\n\naction_state[\"action\"] = discrete_array_element(low=-5, high=5)\n\n\nuser = CarefulPointer(override_policy=(BasePolicy, {\"action_state\": action_state}))\n\nbundle = Bundle(task=task, user=user, assistant=unitcdgain, reset_go_to=1)\n\nobservation = bundle.reset()\n\n\n\n\nCode\nprint(observation)\n\n\n----------------  -----------  -------------------------  -------------------\ngame_info         turn_index   1                          CatSet(4) - int8\n                  round_index  0                          Numeric() - int64\ntask_state        position     10                         Numeric() - int64\n                  targets      [ 1  5  7 13 14 15 19 22]  Numeric(8,) - int64\nuser_state        goal         15                         Numeric() - int64\nuser_action       action       0                          Numeric() - int64\nassistant_action  action       1                          CatSet(2) - int64\n----------------  -----------  -------------------------  -------------------\n\n\nLink to run it in Binder:\n\n\n\nBinder"
  },
  {
    "objectID": "python_tutorials/quickstart.html",
    "href": "python_tutorials/quickstart.html",
    "title": "Python demo using ELFI",
    "section": "",
    "text": "Quickstart\nFirst ensure you have installed Python 3.5 (or greater) and ELFI. After installation you can start using ELFI:\n\n\nCode\nimport elfi\n\n\nELFI includes an easy to use generative modeling syntax, where the generative model is specified as a directed acyclic graph (DAG). Let’s create two prior nodes:\n\nwe use the code-fold option discussed above, so the user can always see this code\n\n\nmu = elfi.Prior('uniform', -2, 4)\nsigma = elfi.Prior('uniform', 1, 4)\n\nThe above would create two prior nodes, a uniform distribution from -2 to 2 for the mean mu and another uniform distribution from 1 to 5 for the standard deviation sigma. All distributions from scipy.stats are available.\nFor likelihood-free models we typically need to define a simulator and summary statistics for the data. As an example, lets define the simulator as 30 draws from a Gaussian distribution with a given mean and standard deviation. Let’s use mean and variance as our summaries:\n\n\nCode\nimport scipy.stats as ss\nimport numpy as np\n\ndef simulator(mu, sigma, batch_size=1, random_state=None):\n    mu, sigma = np.atleast_1d(mu, sigma)\n    return ss.norm.rvs(mu[:, None], sigma[:, None], size=(batch_size, 30), random_state=random_state)\n\ndef mean(y):\n    return np.mean(y, axis=1)\n\ndef var(y):\n    return np.var(y, axis=1)\n\n\nLet’s now assume we have some observed data y0 (here we just create some with the simulator):\n\n\nCode\n# Set the generating parameters that we will try to infer\nmean0 = 1\nstd0 = 3\n\n# Generate some data (using a fixed seed here)\nnp.random.seed(20170525) \ny0 = simulator(mean0, std0)\nprint(y0)\n\n\n[[ 3.7990926   1.49411834  0.90999905  2.46088006 -0.10696721  0.80490023\n   0.7413415  -5.07258261  0.89397268  3.55462229  0.45888389 -3.31930036\n  -0.55378741  3.00865492  1.59394854 -3.37065996  5.03883749 -2.73279084\n   6.10128027  5.09388631  1.90079255 -1.7161259   3.86821266  0.4963219\n   1.64594033 -2.51620566 -0.83601666  2.68225112  2.75598375 -6.02538356]]\n\n\nNow we have all the components needed. Let’s complete our model by adding the simulator, the observed data, summaries and a distance to our model:\n\n\nCode\n# Add the simulator node and observed data to the model\nsim = elfi.Simulator(simulator, mu, sigma, observed=y0)\n\n# Add summary statistics to the model\nS1 = elfi.Summary(mean, sim)\nS2 = elfi.Summary(var, sim)\n\n# Specify distance as euclidean between summary vectors (S1, S2) from simulated and\n# observed data\nd = elfi.Distance('euclidean', S1, S2)\n\n\nIf you have graphviz installed to your system, you can also visualize the model:\n\n\nCode\n# Plot the complete model (requires graphviz)\nelfi.draw(d)\n\n\n\n\n\n.. Note:: The automatic naming of nodes may not work in all environments e.g. in interactive Python shells. You can alternatively provide a name argument for the nodes, e.g. S1 = elfi.Summary(mean, sim, name='S1').\nWe can try to infer the true generating parameters mean0 and std0 above with any of ELFI’s inference methods. Let’s use ABC Rejection sampling and sample 1000 samples from the approximate posterior using threshold value 0.5:\n\n\nCode\nrej = elfi.Rejection(d, batch_size=10000, seed=30052017)\nres = rej.sample(1000, threshold=.5)\nprint(res)\n\n\nProgress [==================================================] 100.0% Complete\nMethod: Rejection\nNumber of samples: 1000\nNumber of simulations: 130000\nThreshold: 0.481\nSample means: _prior_8336: 0.718, _prior_9a96: -0.0234, _prior_f596: 3, _prior_ff70: 3.12, mu: 0.00722, sigma: 3.01\n\n\n\nLet’s plot also the marginal distributions for the parameters:\n\n\nCode\nimport matplotlib.pyplot as plt\nres.plot_marginals()\nplt.show()"
  },
  {
    "objectID": "python_tutorials/quartomarkdown.html",
    "href": "python_tutorials/quartomarkdown.html",
    "title": "Quarto document",
    "section": "",
    "text": "import numpy as np\n\nprint(1+2)\n\n3"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Available tutorials",
    "section": "",
    "text": "Available posts:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia / Quarto example\n\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2022\n\n\nDavid Mawdsley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPOMDPs in Julia\n\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2022\n\n\nDavid Mawdsley\n\n\n\n\n\n\n  \n\n\n\n\nPython Notebook demo\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2022\n\n\nDavid Mawdsley\n\n\n\n\n\n\n  \n\n\n\n\nPython demo using ELFI\n\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2022\n\n\nELFI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto document\n\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2022\n\n\nDavid Mawdsley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "julia_tutorials/julia_example.html",
    "href": "julia_tutorials/julia_example.html",
    "title": "Julia / Quarto example",
    "section": "",
    "text": "This uses the example from the Quarto homepage\n\n\nPlot function pair (x(u), y(u)). See Figure 1 for an example.\n\nusing Plots\n\nplot(sin, \n     x->sin(2x), \n     0, \n     2π, \n     leg=false, \n     fill=(0,:lavender))\n\n\n\n\nFigure 1: Parametric Plots"
  },
  {
    "objectID": "julia_tutorials/pomdps.html",
    "href": "julia_tutorials/pomdps.html",
    "title": "POMDPs in Julia",
    "section": "",
    "text": "This is a longer Quarto document playing around with the POMDPs package in Julia to see how it works.\nUsing the quick start for POMDPs as an example\n\nusing POMDPs, QuickPOMDPs, POMDPModelTools, POMDPSimulators, QMDP\n\nm = QuickPOMDP(\n    states = [\"left\", \"right\"],\n    actions = [\"left\", \"right\", \"listen\"],\n    observations = [\"left\", \"right\"],\n    initialstate = Uniform([\"left\", \"right\"]),\n    discount = 0.95,\n\n    transition = function (s, a)\n        if a == \"listen\"\n            return Deterministic(s) # tiger stays behind the same door\n        else # a door is opened\n            return Uniform([\"left\", \"right\"]) # reset\n        end\n    end,\n\n    observation = function (s, a, sp)\n        if a == \"listen\"\n            if sp == \"left\"\n                return SparseCat([\"left\", \"right\"], [0.85, 0.15]) # sparse categorical distribution\n            else\n                return SparseCat([\"right\", \"left\"], [0.85, 0.15])\n            end\n        else\n            return Uniform([\"left\", \"right\"])\n        end\n    end,\n\n    reward = function (s, a)\n        if a == \"listen\"\n            return -1.0\n        elseif s == a # the tiger was found\n            return -100.0\n        else # the tiger was escaped\n            return 10.0\n        end\n    end\n)\n\nsolver = QMDPSolver()\npolicy = solve(solver, m)\n\nrsum = 0.0\nfor (s,b,a,o,r) in stepthrough(m, policy, \"s,b,a,o,r\", max_steps=10)\n    println(\"s: $s, b: $([s=>pdf(b,s) for s in states(m)]), a: $a, o: $o\")\n    global rsum += r\nend\nprintln(\"Undiscounted reward was $rsum.\")\n\ns: right, b: [\"left\" => 0.5, \"right\" => 0.5], a: listen, o: right\ns: right, b: [\"left\" => 0.15, \"right\" => 0.85], a: listen, o: right\ns: right, b: [\"left\" => 0.0302013422818792, \"right\" => 0.9697986577181208], a: left, o: right\ns: left, b: [\"left\" => 0.5, \"right\" => 0.5], a: listen, o: left\ns: left, b: [\"left\" => 0.85, \"right\" => 0.15], a: listen, o: left\ns: left, b: [\"left\" => 0.9697986577181208, \"right\" => 0.0302013422818792], a: right, o: right\ns: right, b: [\"left\" => 0.5, \"right\" => 0.5], a: listen, o: right\ns: right, b: [\"left\" => 0.15, \"right\" => 0.85], a: listen, o: right\ns: right, b: [\"left\" => 0.0302013422818792, \"right\" => 0.9697986577181208], a: left, o: right\ns: right, b: [\"left\" => 0.5, \"right\" => 0.5], a: listen, o: right\nUndiscounted reward was 23.0."
  }
]